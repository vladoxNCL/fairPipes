{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Optimisation Thru Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "### Imports ###\n",
    "###############\n",
    "\n",
    "# %cd ~/Dropbox/Newcastle/PhD/Notebooks/fair_pipes\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "from copy import deepcopy\n",
    "from itertools import cycle, permutations\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.tree import DecisionTreeClassifier as dt\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "# clf = dt()\n",
    "clf = lr(max_iter=10000)\n",
    "savepath = 'datasets/'\n",
    "\n",
    "#############################\n",
    "### Fairness Dictionaries ###\n",
    "#############################\n",
    "\n",
    "def dp(test):\n",
    "    '''Demographic Parity'''\n",
    "    upr = test[test.pa == 0].pred.value_counts(normalize=True).get(1, 0)\n",
    "    fpr = test[test.pa == 1].pred.value_counts(normalize=True).get(1, 0)\n",
    "    dp = abs(fpr - upr)\n",
    "    return dp\n",
    "\n",
    "def dpr(test):\n",
    "    '''Demographic Parity Ratio'''\n",
    "    upr = test[test.pa == 0].pred.value_counts(normalize=True).get(1, 0)\n",
    "    fpr = test[test.pa == 1].pred.value_counts(normalize=True).get(1, 0)\n",
    "    dpr = abs(upr / fpr) if fpr > 0 else 1 if upr == 0 else -1\n",
    "    return dpr\n",
    "\n",
    "def eo(test):\n",
    "    '''Equality of Opportunity'''\n",
    "    f_fnr = test[(test.pa == 1) & (test.label == 1)].pred.value_counts(normalize=True).get(0, 0)\n",
    "    u_fnr = test[(test.pa == 0) & (test.label == 1)].pred.value_counts(normalize=True).get(0, 0)\n",
    "    eo = abs(f_fnr - u_fnr)\n",
    "    return eo\n",
    "\n",
    "def eqOdds(test):\n",
    "    '''Equalised Odds'''\n",
    "    f_fnr = test[(test.pa == 1) & (test.label == 1)].pred.value_counts(normalize=True).get(0, 0)\n",
    "    u_fnr =test[(test.pa == 0) & (test.label == 1)].pred.value_counts(normalize=True).get(0, 0)\n",
    "    f_fpr = test[(test.pa == 1) & (test.label == 0)].pred.value_counts(normalize=True).get(1, 0)\n",
    "    u_fpr = test[(test.pa == 0) & (test.label == 0)].pred.value_counts(normalize=True).get(1, 0)\n",
    "    eq =  abs(f_fnr - u_fnr) + abs(f_fpr - u_fpr)\n",
    "    return eq\n",
    "\n",
    "def accu(test):\n",
    "    return 1 - accuracy_score(test.label, test.pred)\n",
    "def precision(test):\n",
    "    return 1 - precision_score(test.label, test.pred)\n",
    "def recall(test):\n",
    "    return 1 - recall_score(test.label, test.pred)\n",
    "def f1(test):\n",
    "    return 1 - f1_score(test.label, test.pred)\n",
    "def roc_auc(test):\n",
    "    return 1 - roc_auc_score(test.label, test.pred)\n",
    "\n",
    "fair_funcs = {\n",
    "    'dp': dp,\n",
    "    'eo': eo,\n",
    "    'eqOdds': eqOdds,\n",
    "    'accu': accu,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1': f1,\n",
    "    'roc_auc': roc_auc,\n",
    "    'dpr': dpr,\n",
    "}\n",
    "\n",
    "############################\n",
    "### Dataset Dictionaries ###\n",
    "############################\n",
    "\n",
    "income = {\n",
    "    'df': 'income_ne_mv.csv',\n",
    "    'pa': 'sex',\n",
    "    'label': 'label',\n",
    "}\n",
    "\n",
    "compas = {\n",
    "    'df': 'compas_ne_new.csv',\n",
    "    'pa': 'race',\n",
    "    'label': 'is_recid',\n",
    "}\n",
    "\n",
    "german = {\n",
    "    'df': 'german_ne.csv',\n",
    "    'pa': 'gender',\n",
    "    'label': 'label',\n",
    "}\n",
    "\n",
    "titanic = {\n",
    "    'df': 'Titanic_NoEnc.csv',\n",
    "    'pa': 'Sex',\n",
    "    'label': 'Survived'\n",
    "}\n",
    "\n",
    "# Dset dictionary\n",
    "DSets = {\n",
    "    'income': income,\n",
    "    'compas': compas,\n",
    "    'german': german,\n",
    "    'titanic': titanic,\n",
    "}\n",
    "\n",
    "class Identity:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "\n",
    "#########################\n",
    "### Task Dictionaries ###\n",
    "#########################\n",
    "\n",
    "from category_encoders import (\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    TargetEncoder,\n",
    "    LeaveOneOutEncoder,\n",
    "    WOEEncoder,\n",
    "    CountEncoder,\n",
    ")\n",
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import (\n",
    "    SimpleImputer, \n",
    "#     IterativeImputer, \n",
    "#     KNNImputer,\n",
    ")\n",
    "from sklearn.feature_selection import (\n",
    "#     VarianceThreshold,\n",
    "    SelectKBest,\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler,\n",
    "    MinMaxScaler,\n",
    "    Normalizer,\n",
    "    QuantileTransformer,\n",
    ")\n",
    "from imblearn.over_sampling import (\n",
    "    RandomOverSampler,\n",
    "#     SMOTE,\n",
    "#     ADASYN,\n",
    ")\n",
    "from imblearn.under_sampling import (\n",
    "    RandomUnderSampler,\n",
    "#     ClusterCentroids,\n",
    ")\n",
    "\n",
    "def make_encode_dict(df_id, dset_d=DSets):\n",
    "    global to_encode\n",
    "    global encode_d\n",
    "    df_name = dset_d[df_id]['df']\n",
    "    df = pd.read_csv(savepath + df_name)\n",
    "    to_encode = list(df.select_dtypes(include='object').columns)\n",
    "    encode_d = {\n",
    "        'onehot_enc': OneHotEncoder(cols=to_encode, drop_invariant=True),\n",
    "        'ordinal_enc': OrdinalEncoder(cols=to_encode, drop_invariant=True),\n",
    "        'target_enc': TargetEncoder(cols=to_encode, drop_invariant=True),\n",
    "        'loo_enc': LeaveOneOutEncoder(cols=to_encode, drop_invariant=True),\n",
    "        'woe_enc': WOEEncoder(cols=to_encode, drop_invariant=True),\n",
    "        'count_enc': CountEncoder(cols=to_encode, drop_invariant=True),\n",
    "    }\n",
    "make_encode_dict('income')\n",
    "\n",
    "impute_d = {\n",
    "    'mean_imp': SimpleImputer(strategy='mean'),\n",
    "    'median_imp': SimpleImputer(strategy='median'),\n",
    "    'most_freq_imp': SimpleImputer(strategy='most_frequent'),\n",
    "#     'iter_imp': IterativeImputer(),\n",
    "#     'knn_imp': KNNImputer(),\n",
    "}\n",
    "    \n",
    "def make_fSel_dict(df_id, dset_d=DSets):\n",
    "    df_name = dset_d[df_id]['df']\n",
    "    df = pd.read_csv(savepath + df_name)\n",
    "    n_feats = df.shape[1] - 1\n",
    "    n_sel = 10 if n_feats > 10 else n_feats - 1\n",
    "    global fSel_d\n",
    "    fSel_d = {\n",
    "        'no_sel': Identity(),\n",
    "    #     'var_sel': VarianceThreshold(0.1),\n",
    "        'k_best_sel': SelectKBest(k=n_sel),\n",
    "    }\n",
    "make_fSel_dict('income')\n",
    "\n",
    "scale_d = {\n",
    "    'no_scale': Identity(),\n",
    "    'max_abs_scale': MaxAbsScaler(),\n",
    "    'min_max_scale': MinMaxScaler(),\n",
    "    'norm_scale': Normalizer(),\n",
    "    'quant_scale': QuantileTransformer(),\n",
    "}\n",
    "\n",
    "sample_d = {\n",
    "    'no_samp': Identity(),\n",
    "    'under_samp': RandomUnderSampler(),\n",
    "    'over_samp': RandomOverSampler(),\n",
    "#     'cluster_samp': ClusterCentroids(),\n",
    "#     'adasyn_samp': ADASYN(),\n",
    "#     'smote_samp': SMOTE(),\n",
    "}\n",
    "\n",
    "###############################\n",
    "### Preprocessing Functions ###\n",
    "###############################\n",
    "\n",
    "def relab(name, dset_d=DSets, savepath=savepath):\n",
    "    '''Rename PA and label adequately'''\n",
    "    df = pd.read_csv(savepath + dset_d[name]['df'], header=0)\n",
    "    pa = dset_d[name]['pa']\n",
    "    label = dset_d[name]['label']\n",
    "    df.rename(columns={pa:'pa', label:'label'}, inplace=True)\n",
    "    \n",
    "    # Change PA to 0, 1 for unfav, fav resp.\n",
    "    prs = {s: df[df.pa == s].label.value_counts(normalize=True)[1]\n",
    "           for s in df.pa.unique()}\n",
    "    df.pa = [1 if s == max(prs) else 0 for s in df.pa]\n",
    "    return df\n",
    "\n",
    "def tt_split(df, test_frac=0.2):\n",
    "    test = df.dropna().sample(frac=test_frac)\n",
    "    train = df.drop(test.index)\n",
    "    tt = {'train': train, 'test': test}\n",
    "    return tt\n",
    "\n",
    "def preprocess(df, task_list):\n",
    "    df_mod = df.copy()\n",
    "    \n",
    "    for task in task_list:\n",
    "        X = df_mod.drop('label', axis=1).dropna(axis=1, how='all')\n",
    "        y = df_mod.label.values\n",
    "        pa = df_mod.pa\n",
    "        cols = X.columns\n",
    "        df_fit = task.fit(X, y)\n",
    "        # General case\n",
    "        if not hasattr(task, 'fit_resample'):\n",
    "            tX = df_fit.transform(X)\n",
    "        # Special case for resampling methods\n",
    "        else:\n",
    "            tX, y = task.fit_resample(X, y)\n",
    "        # Special cases for column modifiers\n",
    "        if hasattr(df_fit, 'get_feature_names'):\n",
    "            try:\n",
    "                cols = df_fit.get_feature_names(input_features=cols)\n",
    "            except (NameError, TypeError):\n",
    "                cols = df_fit.get_feature_names()\n",
    "        elif hasattr(df_fit, 'get_support'):\n",
    "            sup = df_fit.get_support(indices=True)\n",
    "            cols = [cols[s] for s in sup]\n",
    "        df_mod = pd.DataFrame(tX, columns=cols)\n",
    "        if not 'pa' in df_mod.columns:\n",
    "            df_mod['pa'] = pa\n",
    "        df_mod['label'] = y    \n",
    "        \n",
    "    return df_mod\n",
    "\n",
    "def predict(train, test, clf=clf):\n",
    "    '''Add predictions to test made by train-learned classifier'''\n",
    "    tst = test.copy()\n",
    "    X_train = train.drop(['label',], axis=1)\n",
    "    y_train = train.label\n",
    "    X_test = tst.drop(['label',], axis=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    tst['pred'] = clf.predict(X_test)\n",
    "    return tst\n",
    "\n",
    "def get_fairness(df, prep_d, fair_d=fair_funcs, cv=False, clf=clf):\n",
    "    '''Get fairness metrics on test resulting from preprocessing train with the tasks in prep_d'''\n",
    "    \n",
    "    preps = {p: preprocess(df, prep_d[p]) for p in prep_d}\n",
    "    \n",
    "    if cv:\n",
    "        preds = {}\n",
    "        for p in preps:\n",
    "            prp = preps[p].copy()\n",
    "            X = prp.drop('label', axis=1)\n",
    "            y = prp.label\n",
    "            cv_pred = cross_val_predict(clf, X, y, cv=4, n_jobs=-1)\n",
    "            preds[p] = prp.assign(pred=cv_pred)\n",
    "    else:\n",
    "        splits = {p: tt_split(preps[p]) for p in preps}\n",
    "        outs = {'trains': {p: splits[p]['train'] for p in preps},\n",
    "                'tests': {p: splits[p]['test'] for p in preps}}\n",
    "        preds = {p: predict(outs['trains'][p], outs['tests'][p])\n",
    "                 for p in preps}\n",
    "    \n",
    "    fair_metrics = {f: {p: fair_d[f](preds[p]) for p in preds}\n",
    "                    for f in fair_d}\n",
    "    \n",
    "    return fair_metrics\n",
    "\n",
    "###################################\n",
    "### Genetic Algorithm Functions ###\n",
    "###################################\n",
    "\n",
    "def make_task_perms(tasks):\n",
    "    global task_perms\n",
    "    task_perms = list(permutations(tasks))\n",
    "    return\n",
    "\n",
    "def decode_pipes(pipes_d):\n",
    "    end_d = {'fSel': 2, 'sample': 3, 'scale': 4}\n",
    "    dec_pipes = {}\n",
    "    for k in pipes_d:\n",
    "        perm = pipes_d[k][-1]\n",
    "        indices = [end_d[perm[i]] for i in range(len(perm))]\n",
    "        dec_pipes[k] = pipes_d[k][0:2] + [pipes_d[k][i] for i in indices]\n",
    "    return dec_pipes\n",
    "\n",
    "def rep_avg(df_id, prep_d, to_opt, cv, gen, n_rep):\n",
    "    '''Repeat and average the fairness metrics'''\n",
    "    df = relab(df_id)\n",
    "    \n",
    "    # Experiment loop\n",
    "    reps = []\n",
    "    for i in range(n_rep):\n",
    "        reps.append(get_fairness(df=df, prep_d=prep_d, cv=cv))\n",
    "    \n",
    "#     Average-out all experiments\n",
    "    results = {f: {g: sum([reps[i][f][g] for i in range(n_rep)]) / n_rep\n",
    "                   for g in reps[0][f]}\n",
    "               for f in reps[0]}\n",
    "    \n",
    "    metrics = results.keys()\n",
    "    metrics_lc = ['dp', 'eo', 'eqOdds', 'accu']\n",
    "    pipe_names = results['dp'].keys()\n",
    "    mod_results = {n: {m: results[m][n] for m in metrics} for n in pipe_names}\n",
    "    \n",
    "    for n in mod_results:\n",
    "        mod_results[n]['gen'] = gen\n",
    "        mod_results[n]['lin_comb'] = sum([coef*mod_results[n][met]\n",
    "                                          for coef, met in zip(to_opt, metrics_lc)])\n",
    "    \n",
    "    return mod_results\n",
    "\n",
    "def make_results_df(results):\n",
    "    df = pd.DataFrame.from_dict(results, orient='index')\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    df.rename(columns={'index': 'pipe'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Generate initial population\n",
    "def random_pop(df_id, pop_size, to_opt, cv, n_rep):\n",
    "    pipes = {}\n",
    "    for i in range(pop_size):\n",
    "        encode_name = random.choice(list(encode_d))\n",
    "        impute_name = random.choice(list(impute_d))\n",
    "        fSel_name = random.choice(list(fSel_d))\n",
    "        sample_name = random.choice(list(sample_d))\n",
    "        scale_name = random.choice(list(scale_d))\n",
    "        p_tasks = {'fSel': fSel_name, 'sample':sample_name, 'scale': scale_name}\n",
    "        perm = random.choice(task_perms)\n",
    "        name_list = [encode_name, impute_name] + [p_tasks[name] for name in perm]\n",
    "        pipe_name = ','.join(name_list)\n",
    "        pipe_start = [encode_d[encode_name], impute_d[impute_name]]\n",
    "        pipe_end = [fSel_d[fSel_name], sample_d[sample_name], scale_d[scale_name], perm]\n",
    "        pipe = pipe_start + pipe_end\n",
    "        pipes[pipe_name] = pipe\n",
    "    \n",
    "    prep_d = decode_pipes(pipes)\n",
    "    res = rep_avg(df_id, prep_d, to_opt, cv, 0, n_rep)\n",
    "    pop = {'pipes': pipes, 'results': res}\n",
    "    return pop\n",
    "\n",
    "def crossover(parents):\n",
    "    splits, pipes = [], []\n",
    "    for k in parents:\n",
    "        splits.append(k.split(','))\n",
    "        pipes.append(parents[k])\n",
    "    co_point = random.randrange(len(pipes[0]))\n",
    "    c_splits, c_pipes = deepcopy(splits), deepcopy(pipes)\n",
    "    perms = [pipes[i][-1] for i in range(2)]\n",
    "    permute_ds = {i: {perms[i][j]: splits[i][len(splits) - len(perms[i]) + j - 2]\n",
    "                      for j in range(len(perms[i]))} for i in range(2)}\n",
    "    task_index_d = {2: 'fSel', 3: 'sample', 4: 'scale'}\n",
    "    \n",
    "    for i in range(2):\n",
    "        k = (i + 1) % 2\n",
    "        c_pipes[i][co_point] = pipes[k][co_point]\n",
    "        if co_point == (len(pipes[0]) - 1):\n",
    "            perm = perms[k]\n",
    "            const_length = len(c_splits[i]) - len(perm)\n",
    "            c_splits[i] = c_splits[i][:const_length] + [permute_ds[i][key] for key in perm]\n",
    "        elif co_point in [2, 3, 4]:\n",
    "            task_type = task_index_d[co_point]\n",
    "            original_task = permute_ds[i][task_type]\n",
    "            ot_index = c_splits[i].index(original_task)\n",
    "            new_task = permute_ds[k][task_type]\n",
    "            c_splits[i][ot_index] = new_task\n",
    "        else:\n",
    "            c_splits[i][co_point] = splits[k][co_point]\n",
    "    \n",
    "    c_joins = [','.join(c_splits[i]) for i in range(2)]\n",
    "    children = {c_joins[i]: c_pipes[i] for i in range(2)}\n",
    "    return children\n",
    "\n",
    "def mutate(population, mut_ratio):\n",
    "    pop = population.copy()\n",
    "    index_dicts = {0: encode_d, 1: impute_d, 2: fSel_d, 3: sample_d, 4: scale_d}\n",
    "    task_index_d = {2: 'fSel', 3: 'sample', 4: 'scale'}\n",
    "    to_remove = []\n",
    "    to_add = {}\n",
    "    \n",
    "    for k in pop:\n",
    "        split_key = k.split(',')\n",
    "        tasks = pop[k].copy()\n",
    "        chrom_r = range(len(pop[k]))\n",
    "        \n",
    "        for i in chrom_r:\n",
    "            ru = random.uniform(0, 1)\n",
    "            \n",
    "            if ru < mut_ratio:\n",
    "                to_remove.append(k)\n",
    "                \n",
    "                if i in range(2):\n",
    "                    new_task_name = random.choice([x for x in index_dicts[i] if x != split_key[i]])\n",
    "                    new_task = index_dicts[i][new_task_name]\n",
    "                    split_key[i] = new_task_name\n",
    "                    tasks[i] = new_task\n",
    "                \n",
    "                elif i in range(2, chrom_r[-1]):\n",
    "                    task_type = task_index_d[i]\n",
    "                    split_index = tasks[5].index(task_type) + 2\n",
    "                    new_task_name = random.choice([x for x in index_dicts[i] if x != split_key[split_index]])\n",
    "                    new_task = index_dicts[i][new_task_name]\n",
    "                    split_key[split_index] = new_task_name\n",
    "                    tasks[i] = new_task\n",
    "                \n",
    "                else:\n",
    "                    sk = split_key.copy()\n",
    "                    perm = tasks[i]\n",
    "                    new_perm = random.choice([t for t in task_perms if t != perm])\n",
    "                    for v in task_index_d.values():\n",
    "                        sk[2+new_perm.index(v)] = split_key[2+perm.index(v)]\n",
    "                    tasks[i] = new_perm\n",
    "                    split_key = sk\n",
    "                \n",
    "        new_name = ','.join(split_key)\n",
    "        new_pipe = tasks\n",
    "        to_add[new_name] = new_pipe\n",
    "    \n",
    "    for r in to_remove:\n",
    "        pop.pop(r, None)\n",
    "    pop.update(to_add)\n",
    "    return pop\n",
    "\n",
    "def next_pop(df_id, pop_dict, co_ratio, mut_ratio, to_opt, cv, gen, n_rep):\n",
    "    pop = pop_dict['pipes']\n",
    "    pop_size = len(pop)\n",
    "    tnc = int(co_ratio * pop_size)\n",
    "    n_children = tnc if tnc % 2 == 0 else tnc - 1 \n",
    "    n_reps = int(n_children / 2)\n",
    "    n_survivors = pop_size - n_children - 1\n",
    "    \n",
    "    res = pop_dict['results']\n",
    "    sorted_res = {k: v for k, v in sorted(res.items(),\n",
    "                                          key=lambda item: item[1]['lin_comb'])}\n",
    "    sorted_pipe_names = list(sorted_res)\n",
    "    \n",
    "    # Generate Children\n",
    "    next_pop = {}\n",
    "    rep_den = sum(range(pop_size+1))\n",
    "    rep_probs = [(pop_size - i) / rep_den for i in range(pop_size)]\n",
    "    for r in range(n_reps):\n",
    "        parent_names = list(np.random.choice(sorted_pipe_names, size=2,\n",
    "                                             replace=False, p=rep_probs))\n",
    "        parents = {n: pop[n] for n in parent_names}\n",
    "        children = crossover(parents)\n",
    "        next_pop.update(children)\n",
    "    \n",
    "    # Add survivors from prev generation\n",
    "    p_surv_names = sorted_pipe_names[1:]\n",
    "    surv_den = sum(range(pop_size))\n",
    "    surv_probs = [(pop_size - 1 - i) / surv_den for i in range(pop_size - 1)]\n",
    "    surv_names = list(np.random.choice(p_surv_names, size=n_survivors,\n",
    "                                       replace=False, p=surv_probs))\n",
    "    survivors = {n: pop[n] for n in surv_names}\n",
    "    next_pop.update(survivors)\n",
    "    \n",
    "    # Mutate next_pop\n",
    "    next_pop = mutate(next_pop, mut_ratio)\n",
    "    \n",
    "    # Save non-mutated best pipe from prev generation\n",
    "    top_pipe = sorted_pipe_names[0]\n",
    "    next_pop[top_pipe] = pop[top_pipe]\n",
    "    \n",
    "    # Complete gen with more mutations if necessary\n",
    "    while len(next_pop) < pop_size:\n",
    "        n_needed = pop_size - len(next_pop)\n",
    "        p_surv_names = sorted_pipe_names\n",
    "        surv_den = sum(range(pop_size + 1))\n",
    "        surv_probs = [(pop_size - i) / surv_den for i in range(pop_size)]\n",
    "        surv_names = list(np.random.choice(p_surv_names, size=n_needed,\n",
    "                                           replace=False, p=surv_probs))\n",
    "        survivors = {n: pop[n] for n in surv_names}\n",
    "        mut_survs = mutate(survivors, mut_ratio)\n",
    "        next_pop.update(mut_survs)\n",
    "    \n",
    "    # Get results for new gen\n",
    "    prep_d = decode_pipes(next_pop)\n",
    "    new_res = rep_avg(df_id, prep_d, to_opt, cv, gen, n_rep)\n",
    "    pop = {'pipes': next_pop, 'results': new_res}\n",
    "    return pop\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def gen_pipe_opt(df_id, opt_list=[(1, 0, 0, 0)], pop_size=10, n_gens=10,\n",
    "                 co_ratio=0.6, mut_ratio=0.005, cv=False, n_rep=1):\n",
    "    \n",
    "    # recreate necessary dicts and lists\n",
    "    make_encode_dict(df_id)\n",
    "    make_fSel_dict(df_id)\n",
    "    tasks = ['fSel', 'scale', 'sample']\n",
    "    make_task_perms(tasks)\n",
    "    \n",
    "    metrics = ['dp', 'eo', 'eqOdds', 'accu']\n",
    "    opt_cycle = cycle(opt_list)\n",
    "    to_opt = next(opt_cycle)\n",
    "    pop_dict = random_pop(df_id, pop_size, to_opt, cv, n_rep)\n",
    "    results_df = make_results_df(pop_dict['results'])\n",
    "    \n",
    "    for i in trange(n_gens, desc='Generation'):\n",
    "        to_opt = next(opt_cycle)\n",
    "        pop_dict = next_pop(df_id, pop_dict, co_ratio, mut_ratio,\n",
    "                            to_opt, cv, i+1, n_rep)\n",
    "        partial_df = make_results_df(pop_dict['results'])\n",
    "        results_df = results_df.append(partial_df, ignore_index=True)\n",
    "    out = {'last_gen': pop_dict, 'results': results_df}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Search Metrics in Ground Truth Hack #\n",
    "#######################################\n",
    "\n",
    "def rep_avg(df_id, prep_d, to_opt, cv, gen, n_rep):\n",
    "    \n",
    "    ex_df = pd.read_csv(f'results/{df_id}_cv_exhaustive.csv', index_col='pipe')\n",
    "    metrics = ('dp', 'dpr', 'eo', 'eqOdds', 'accu', 'precision', 'recall', 'f1', 'roc_auc')\n",
    "    metrics_lc = ['dp', 'eo', 'eqOdds', 'accu']\n",
    "    pipe_names = list(prep_d.keys())\n",
    "    mod_results = {n: {m: ex_df.loc[n, m] for m in metrics} for n in pipe_names}\n",
    "    \n",
    "    for n in mod_results:\n",
    "        mod_results[n]['gen'] = gen\n",
    "        mod_results[n]['lin_comb'] = sum([coef*mod_results[n][met]\n",
    "                                          for coef, met in zip(to_opt, metrics_lc)])\n",
    "    \n",
    "    return mod_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "### Test Run ###\n",
    "################\n",
    "\n",
    "ol = [(3, 0, 0, 7)]\n",
    "seed_everything(26)\n",
    "test_exp = gen_pipe_opt('income', pop_size=4, n_gens=1, co_ratio=0.5, mut_ratio=0.01, opt_list=ol, cv=False)\n",
    "test_exp['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "from multiprocessing import Pool  \n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "# datasets = ['income', 'compas', 'german', 'titanic']\n",
    "datasets = ['compas']\n",
    "seed_everything(42)\n",
    "seeds = [random.randint(1, 100000) for i in range(64)]\n",
    "dset_seed = list(product(datasets, seeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repetition(p):\n",
    "    dataset = p[0]\n",
    "    seed = p[1]\n",
    "    batch_size = 20\n",
    "\n",
    "    make_encode_dict(dataset)\n",
    "    make_fSel_dict(dataset)\n",
    "\n",
    "    l = list(product(encode_d, impute_d))\n",
    "    l = [list(e) for e in l]\n",
    "\n",
    "    m = list(product(fSel_d, sample_d, scale_d))\n",
    "    m = [list(permutations(e)) for e in m]\n",
    "    m = [tup for lst in m for tup in lst]\n",
    "    m = [list(e) for e in m]\n",
    "\n",
    "    names = list(product(l, m))\n",
    "    names = [tuple(e[0] + e[1]) for e in names]\n",
    "\n",
    "    l = list(product(encode_d.values(), impute_d.values()))\n",
    "    l = [list(e) for e in l]\n",
    "\n",
    "    m = list(product(fSel_d.values(), sample_d.values(), scale_d.values()))\n",
    "    m = [list(permutations(e)) for e in m]\n",
    "    m = [tup for lst in m for tup in lst]\n",
    "    m = [list(e) for e in m]\n",
    "\n",
    "    pipes = list(product(l, m))\n",
    "    pipes = [e[0] + e[1] for e in pipes]\n",
    "\n",
    "    names = [names[i: i + batch_size] for i in range(0, len(names), batch_size)]\n",
    "    pipes = [pipes[i: i + batch_size] for i in range(0, len(pipes), batch_size)]\n",
    "\n",
    "    pipe_dicts = [{n: p for n, p in zip(curr_name, curr_pipe)}\n",
    "                  for curr_name, curr_pipe in zip(names, pipes)]\n",
    "\n",
    "    seed_everything(seed)\n",
    "    all_res = [rep_avg(df_id=dataset, prep_d=p, to_opt=(0,0,0,1),\n",
    "                       cv=True, gen=0, n_rep=1)\n",
    "               for p in tqdm(pipe_dicts, desc='Batch')]\n",
    "\n",
    "    dfs = [pd.DataFrame.from_dict(e, orient='index') for e in all_res]\n",
    "    dfs = [e.reset_index(level=[0,1,2,3,4]) for e in dfs]\n",
    "\n",
    "    for e in dfs:\n",
    "        e['pipe'] = (e['level_0']\n",
    "                     + ',' + e['level_1']\n",
    "                     + ',' + e['level_2']\n",
    "                     + ',' + e['level_3']\n",
    "                     + ',' + e['level_4'])\n",
    "\n",
    "    levels = [f'level_{str(i)}' for i in range(5)]\n",
    "    dfs = [e.drop(columns=levels) for e in dfs]\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    cols = ['pipe', 'dp', 'dpr', 'eo', 'eqOdds', 'accu', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    df = df[cols]\n",
    "    df.to_csv(f'results/{dataset}_cv_exhaustive_{seed}.csv', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool() as p:\n",
    "        with tqdm(total=len(dset_seed), desc='Repetition') as pbar:\n",
    "            for i, _ in enumerate(p.imap_unordered(repetition, dset_seed)):\n",
    "                pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "### Generate Ground Truth ###\n",
    "#############################\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "dsets = ['compas', 'german', 'income', 'titanic']\n",
    "\n",
    "for d in tqdm(dsets, desc='Dataset'):\n",
    "    dataset = d\n",
    "    batch_size = 20\n",
    "\n",
    "    make_encode_dict(dataset)\n",
    "    make_fSel_dict(dataset)\n",
    "\n",
    "    l = list(product(encode_d, impute_d))\n",
    "    l = [list(e) for e in l]\n",
    "\n",
    "    m = list(product(fSel_d, sample_d, scale_d))\n",
    "    m = [list(permutations(e)) for e in m]\n",
    "    m = [tup for lst in m for tup in lst]\n",
    "    m = [list(e) for e in m]\n",
    "\n",
    "    names = list(product(l, m))\n",
    "    names = [tuple(e[0] + e[1]) for e in names]\n",
    "\n",
    "    l = list(product(encode_d.values(), impute_d.values()))\n",
    "    l = [list(e) for e in l]\n",
    "\n",
    "    m = list(product(fSel_d.values(), sample_d.values(), scale_d.values()))\n",
    "    m = [list(permutations(e)) for e in m]\n",
    "    m = [tup for lst in m for tup in lst]\n",
    "    m = [list(e) for e in m]\n",
    "\n",
    "    pipes = list(product(l, m))\n",
    "    pipes = [e[0] + e[1] for e in pipes]\n",
    "\n",
    "    names = [names[i: i + batch_size] for i in range(0, len(names), batch_size)]\n",
    "    pipes = [pipes[i: i + batch_size] for i in range(0, len(pipes), batch_size)]\n",
    "\n",
    "    pipe_dicts = [{n: p for n, p in zip(curr_name, curr_pipe)}\n",
    "                  for curr_name, curr_pipe in zip(names, pipes)]\n",
    "\n",
    "    seed_everything()\n",
    "    all_res = [rep_avg(df_id=dataset, prep_d=p, to_opt=(0,0,0,1),\n",
    "                       cv=True, gen=0, n_rep=1)\n",
    "               for p in tqdm(pipe_dicts, desc='Batch')]\n",
    "\n",
    "    dfs = [pd.DataFrame.from_dict(e, orient='index') for e in all_res]\n",
    "    dfs = [e.reset_index(level=[0,1,2,3,4]) for e in dfs]\n",
    "\n",
    "    levels = [f'level_{str(i)}' for i in range(5)]\n",
    "    for e in dfs:\n",
    "#         e['pipe'] = ','.join(levels)\n",
    "       e['pipe'] = ','.join([e[l] for l in levels])\n",
    "\n",
    "    dfs = [e.drop(columns=levels) for e in dfs]\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    cols = ['pipe', 'dp', 'dpr', 'eo', 'eqOdds', 'accu', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    df = df[cols]\n",
    "    df.to_csv(f'results/{dataset}_cv_exhaustive.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [f'num_{str(i)}' for i in range(4)]\n",
    "','.join(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### Ground Truth EDA ###\n",
    "########################\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "sns.set_theme(context='notebook', style='whitegrid', palette='Dark2')\n",
    "sns.set(rc={'figure.figsize':(5, 5)})\n",
    "colormap = mpl.cm.Dark2.colors\n",
    "\n",
    "dset_names = {\n",
    "    'compas': 'COMPAS',\n",
    "    'german': 'German Credit',\n",
    "    'income': 'Adult Income',\n",
    "    'titanic': 'Titanic',\n",
    "}\n",
    "\n",
    "task_names = {\n",
    "    'enc': 'Encoding',\n",
    "    'imp': 'Imputing',\n",
    "    'samp': 'Sampling',\n",
    "    'sel': 'Feature Selection',\n",
    "    'scale': 'Feature Scaling',\n",
    "    'perm': 'Permutation',\n",
    "}\n",
    "\n",
    "def plot_grid(dataset, var, show=False):\n",
    "    # Load data\n",
    "    ex_df = pd.read_csv(f'results/{dataset}_cv_exhaustive.csv')\n",
    "    \n",
    "    # Create directory if necessary\n",
    "    if not os.path.exists(f'plots/exhaustive/{dataset}'):\n",
    "        os.makedirs(f'plots/exhaustive/{dataset}')\n",
    "        \n",
    "    # Create necessary columns\n",
    "    ex_df['splits'] = ex_df['pipe'].str.split(',')\n",
    "    tasks = {'sel': [], 'samp': [], 'scale': []}\n",
    "    perms = []\n",
    "    for i in range(ex_df.shape[0]):\n",
    "        perm = []\n",
    "        for j in range(2,5):\n",
    "            spl = ex_df.splits[i][j].rpartition('_')\n",
    "            k = spl[2]\n",
    "            v = spl[0]\n",
    "            tasks[k].append(v)\n",
    "            perm.append(k)\n",
    "        perm = tuple(perm)\n",
    "        perms.append(perm)\n",
    "    ex_df['enc'] = [ex_df.splits[i][0].rpartition('_')[0]\n",
    "                    for i in range(ex_df.shape[0])]\n",
    "    ex_df['imp'] = [ex_df.splits[i][1].rpartition('_')[0]\n",
    "                    for i in range(ex_df.shape[0])]\n",
    "    for t in tasks:\n",
    "        ex_df[t] = tasks[t]\n",
    "    ex_df['perm'] = perms\n",
    "    ex_df.drop(columns=['splits'], inplace=True)\n",
    "    \n",
    "    # Plot\n",
    "    g = sns.PairGrid(ex_df[['dp', 'eo', 'eqOdds', 'accu', var]],\n",
    "                     hue=var, diag_sharey=False)\n",
    "    g.map_diag(sns.histplot, multiple='stack', bins=20)\n",
    "    g.map_offdiag(sns.scatterplot, alpha=0.2)\n",
    "    g.add_legend(title=task_names[var])\n",
    "    g.fig.subplots_adjust(top=0.95)\n",
    "    g.fig.suptitle(f'{dset_names[dataset]} data, {task_names[var]}', fontsize=16)\n",
    "    plt.savefig(f'plots/exhaustive/{dataset}/encoding.pdf', bbox_inches='tight')\n",
    "    if not show:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "### Generate All PairGrid Plots ###\n",
    "###################################\n",
    "\n",
    "datasets = ['compas', 'german', 'income', 'titanic']\n",
    "task_cats = ['enc', 'imp', 'samp', 'scale', 'sel', 'perm']\n",
    "\n",
    "for dataset in tqdm(datasets, desc='Dataset'):\n",
    "    for task in tqdm(task_cats, desc='Task', leave=False):\n",
    "        plot_grid(dataset, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "### Show Single PairGrid ###\n",
    "############################\n",
    "\n",
    "plot_grid('income', 'scale', show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "from multiprocessing import Pool  \n",
    "\n",
    "datasets = ['compas']\n",
    "# datasets = ['income', 'compas', 'german', 'titanic']\n",
    "seed_everything(42)\n",
    "seeds = [random.randint(1, 100000) for i in range(128)]\n",
    "dset_seed = list(product(datasets, seeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Metric Optimisation\n",
    "all_metrics = [[(1,0,0,0)], [(0,1,0,0)], [(0,0,1,0)], [(0,0,0,1)]]\n",
    "\n",
    "def repetition(p):\n",
    "    dataset = p[0]\n",
    "    seed = p[1]\n",
    "    experiments = {}\n",
    "    \n",
    "    for m in all_metrics:\n",
    "        seed_everything(seed)\n",
    "        experiments[m[0]] = gen_pipe_opt(dataset, pop_size=10, n_gens=20,\n",
    "                                         opt_list=m, cv=True)\n",
    "\n",
    "    exp_dfs = [experiments[k]['results'] for k in experiments]\n",
    "    exp_keys = list(experiments)\n",
    "\n",
    "    for k, e in zip(exp_keys, exp_dfs):\n",
    "        e['opt_metric'] = str(k)\n",
    "\n",
    "    all_metrics_df = pd.concat(exp_dfs, ignore_index=True)\n",
    "    all_metrics_df.drop(columns=['lin_comb'], inplace=True)\n",
    "    all_metrics_df['seed'] = seed\n",
    "    all_metrics_df.to_csv(f'results/repeats/{dataset}_all_metrics_{str(seed)}.csv', index=False)\n",
    "    \n",
    "# for ds in tqdm(dset_seed, desc='Repetition'):\n",
    "#     repetition(ds)\n",
    "if __name__ == '__main__':\n",
    "    with Pool() as p:\n",
    "        with tqdm(total=len(dset_seed), desc='Repetition') as pbar:\n",
    "            for i, _ in enumerate(p.imap_unordered(repetition, dset_seed)):\n",
    "                pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear combinations of DP and Accuracy\n",
    "dp_acc_cc = [[(i, 0, 0, 10-i)] for i in range(11)]\n",
    "\n",
    "def repetition(p):\n",
    "    dataset = p[0]\n",
    "    seed = p[1]\n",
    "    experiments = {}\n",
    "    \n",
    "    for i in range(11):\n",
    "        seed_everything(seed)\n",
    "        experiments[(i, 10-i)] = gen_pipe_opt(dataset, pop_size=10, n_gens=20,\n",
    "                                              opt_list=dp_acc_cc[i], cv=True)\n",
    "\n",
    "    exp_dfs = [experiments[k]['results'] for k in experiments]\n",
    "    exp_keys = list(experiments)\n",
    "\n",
    "    for k, e in zip(exp_keys, exp_dfs):\n",
    "        e['dp_acc_ratio'] = str(k)\n",
    "\n",
    "    dp_acc_df = pd.concat(exp_dfs, ignore_index=True)\n",
    "    dp_acc_df.drop(columns=['lin_comb'], inplace=True)\n",
    "    dp_acc_df['seed'] = seed\n",
    "    dp_acc_df.to_csv(f'results/repeats/{dataset}_dp_acc_ratios_{str(seed)}.csv', index=False)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    with Pool() as p:\n",
    "        with tqdm(total=len(dset_seed), desc='Repetition') as pbar:\n",
    "            for i, _ in enumerate(p.imap_unordered(repetition, dset_seed)):\n",
    "                pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DP first, then Accu\n",
    "dp_acc_reps = [[(1,0,0,0)]*i + [(0,0,0,1)]*(20-i) for i in range(0,21,4)]\n",
    "\n",
    "def repetition(p):\n",
    "    dataset = p[0]\n",
    "    seed = p[1]\n",
    "    experiments = {}\n",
    "    \n",
    "    for i in range(6):\n",
    "        seed_everything(seed)\n",
    "        experiments[(4*i, 20-4*i)] = gen_pipe_opt(dataset, pop_size=10, n_gens=20,\n",
    "                                                  opt_list=dp_acc_reps[i], cv=True)\n",
    "\n",
    "    exp_dfs = [experiments[k]['results'] for k in experiments]\n",
    "    exp_keys = list(experiments)\n",
    "\n",
    "    for k, e in zip(exp_keys, exp_dfs):\n",
    "        e['dp_acc_reps'] = str(k)\n",
    "\n",
    "    dp_acc_reps_df = pd.concat(exp_dfs, ignore_index=True)\n",
    "    dp_acc_reps_df.drop(columns=['lin_comb'], inplace=True)\n",
    "    dp_acc_reps_df['seed'] = seed\n",
    "    dp_acc_reps_df.to_csv(f'results/repeats/{dataset}_dp_acc_reps_{str(seed)}.csv', index=False)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    with Pool() as p:\n",
    "        with tqdm(total=len(dset_seed), desc='Repetition') as pbar:\n",
    "            for i, _ in enumerate(p.imap_unordered(repetition, dset_seed)):\n",
    "                pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accu first, then DP\n",
    "acc_dp_reps = [[(0,0,0,1)]*i + [(1,0,0,0)]*(20-i) for i in range(0,21,4)]\n",
    "\n",
    "def repetition(p):\n",
    "    dataset = p[0]\n",
    "    seed = p[1]\n",
    "    experiments = {}\n",
    "    \n",
    "    for i in range(6):\n",
    "        seed_everything(seed)\n",
    "        experiments[(4*i, 20-4*i)] = gen_pipe_opt(dataset, pop_size=10, n_gens=20,\n",
    "                                                  opt_list=acc_dp_reps[i], cv=True)\n",
    "\n",
    "    exp_dfs = [experiments[k]['results'] for k in experiments]\n",
    "    exp_keys = list(experiments)\n",
    "\n",
    "    for k, e in zip(exp_keys, exp_dfs):\n",
    "        e['dp_acc_reps'] = str(k)\n",
    "\n",
    "    acc_dp_reps_df = pd.concat(exp_dfs, ignore_index=True)\n",
    "    acc_dp_reps_df.drop(columns=['lin_comb'], inplace=True)\n",
    "    acc_dp_reps_df['seed'] = seed\n",
    "    acc_dp_reps_df.to_csv(f'results/repeats/{dataset}_acc_dp_reps_{str(seed)}.csv', index=False)\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    with Pool() as p:\n",
    "        with tqdm(total=len(dset_seed), desc='Repetition') as pbar:\n",
    "            for i, _ in enumerate(p.imap_unordered(repetition, dset_seed)):\n",
    "                pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full 20 gens then switch for both accu and dp\n",
    "reps = [[(0,0,0,1)]*20 + [(1,0,0,0)]*(20), [(1,0,0,0)]*20 + [(0,0,0,1)]*(20)]\n",
    "\n",
    "def repetition(p):\n",
    "    dataset = p[0]\n",
    "    seed = p[1]\n",
    "    experiments = {}\n",
    "    exp_keys = ['accu_dp', 'dp_accu']\n",
    "    \n",
    "    for i in range(2):\n",
    "        seed_everything(seed)\n",
    "        experiments[exp_keys[i]] = gen_pipe_opt(dataset, pop_size=10, n_gens=40,\n",
    "                                                  opt_list=reps[i], cv=True)\n",
    "\n",
    "    exp_dfs = [experiments[k]['results'] for k in experiments]\n",
    "\n",
    "    for k, e in zip(exp_keys, exp_dfs):\n",
    "        e['reps'] = str(k)\n",
    "\n",
    "    acc_dp_reps_df = pd.concat(exp_dfs, ignore_index=True)\n",
    "    acc_dp_reps_df.drop(columns=['lin_comb'], inplace=True)\n",
    "    acc_dp_reps_df['seed'] = seed\n",
    "    acc_dp_reps_df.to_csv(f'results/repeats/{dataset}_reps_{str(seed)}.csv', index=False)\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    with Pool() as p:\n",
    "        with tqdm(total=len(dset_seed), desc='Repetition') as pbar:\n",
    "            for i, _ in enumerate(p.imap_unordered(repetition, dset_seed)):\n",
    "                pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varying Mutation Ratio\n",
    "mut_ratios = [0.001*i for i in range(11)]\n",
    "\n",
    "def repetition(p):\n",
    "    dataset = p[0]\n",
    "    seed = p[1]\n",
    "    experiments = {}\n",
    "    \n",
    "    for m in mut_ratios:\n",
    "        seed_everything(seed)\n",
    "        experiments[m] = gen_pipe_opt(dataset, pop_size=10, n_gens=20,\n",
    "                                      mut_ratio=m, opt_list=[(1,0,0,0)],\n",
    "                                      cv=True)\n",
    "\n",
    "    exp_dfs = [experiments[k]['results'] for k in experiments]\n",
    "    exp_keys = list(experiments)\n",
    "\n",
    "    for k, e in zip(exp_keys, exp_dfs):\n",
    "        e['mut_ratio'] = str(k)\n",
    "\n",
    "    mut_df = pd.concat(exp_dfs, ignore_index=True)\n",
    "    mut_df.drop(columns=['lin_comb'], inplace=True)\n",
    "    mut_df['seed'] = seed\n",
    "    mut_df.to_csv(f'results/repeats/{dataset}_mut_ratios_{str(seed)}.csv', index=False)\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    with Pool() as p:\n",
    "        with tqdm(total=64, desc='Repetition') as pbar:\n",
    "            for i, _ in enumerate(p.imap_unordered(repetition, dset_seed[:64])):\n",
    "                pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
